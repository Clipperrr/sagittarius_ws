{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!pip install tf-keras --upgrade\n",
    "!pip uninstall keras -y  # 卸载现有的Keras 3\n",
    "!pip install -q h5py typing-extensions wheel\n",
    "!pip install -q -U bitsandbytes\n",
    "!pip install -q -U git+https://github.com/huggingface/transformers.git\n",
    "!pip install -q -U git+https://github.com/huggingface/peft.git\n",
    "!pip install -q -U git+https://github.com/huggingface/accelerate.git\n",
    "!pip install -q datasets"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "!nvidia-smi",
   "id": "1c40e08f80a6e934"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "model_id = \"deepseek-ai/deepseek-llm-14b-chat\"\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True, # Activate nested quantization for 4-bit base models (double quantization)\n",
    "    bnb_4bit_quant_type=\"nf4\", # Quantization type (fp4 or nf4), According to QLoRA paper, for training 4-bit base models (e.g. using LoRA adapters) one should use\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "# model = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=bnb_config, device_map={\"\":0})\n",
    "\n",
    "# 模型加载配置\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",  # 自动分配设备（CPU/GPU）\n",
    "    low_cpu_mem_usage=True  # 减少CPU内存占用\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)"
   ],
   "id": "f24e517486b6cfe8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from peft import prepare_model_for_kbit_training\n",
    "\n",
    "model.gradient_checkpointing_enable()\n",
    "model = prepare_model_for_kbit_training(model)"
   ],
   "id": "7ecffb8c6dfecc39"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "# You can try differnt parameter-effient strategy for model trianing, for more info, please check https://github.com/huggingface/peft\n",
    "config = LoraConfig(\n",
    "    r=24,                # 平衡模型容量与数据量\n",
    "    lora_alpha=48,       # alpha=2*r\n",
    "    lora_dropout=0.4,    # 强化正则化\n",
    "    target_modules=[\"q_proj\", \"v_proj\", \"k_proj\"],  # 增加k_proj提升注意力机制灵活性\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "model = get_peft_model(model, config)"
   ],
   "id": "e6b05517931f6fa4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from jinja2 import Template\n",
    "template = Template(tokenizer.chat_template)\n",
    "message = \"Please introduce yourself\"\n",
    "print(f\"message:\\n{message}\\n\")\n",
    "message_send_to_model=template.render(messages=[{\"role\": \"user\", \"content\": message}],bos_token=tokenizer.bos_token,add_generation_prompt=True)\n",
    "print(f\"message_send_to_model:\\n{message_send_to_model}\")"
   ],
   "id": "75af26bde0c66bf0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "template = Template(tokenizer.chat_template)\n",
    "@torch.no_grad()\n",
    "def generate(prompt):\n",
    "    modelInput=template.render(messages=[{\"role\": \"user\", \"content\": prompt}],bos_token= tokenizer.bos_token,add_generation_prompt=True)\n",
    "    print(\"-\"*80)\n",
    "    print(f\"model_input_string:\\n{modelInput}\")\n",
    "    input_ids = tokenizer.encode(modelInput, add_special_tokens=False, return_tensors='pt').to(\"cuda:0\")\n",
    "    outputs = model.generate(input_ids, do_sample=False)\n",
    "    model_return_string = tokenizer.decode(*outputs, skip_special_tokens=False)\n",
    "    print(\"-\"*80)\n",
    "    print(f\"model_return_string:\\n{model_return_string}\")\n",
    "    generated_ids = outputs[:, input_ids.shape[1]:]\n",
    "    generated_text = tokenizer.decode(generated_ids[0], skip_special_tokens=False)\n",
    "    return generated_text\n",
    "\n",
    "query = \"Please introduce yourself\"\n",
    "print(\"-\"*80)\n",
    "print(f\"query:\\n{query}\")\n",
    "response = generate(query)\n",
    "print(\"-\"*80)\n",
    "print(f\"response:\\n{response}\")"
   ],
   "id": "55468140031083ac"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"blended_skill_talk\")\n",
    "dataset = dataset['train'].map(lambda sample: {\"conversations\": [{\"from\": \"human\", \"value\": sample['question']}, {\"from\": \"gpt\", \"value\": sample['answer']}]}, batched=False)"
   ],
   "id": "644220ac1c9e1d0f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from torch.utils.data import random_split\n",
    "train_dataset_size, val_dataset_size = 800, 200\n",
    "train_dataset, val_dataset, _ = random_split(dataset, [train_dataset_size, val_dataset_size, len(dataset)-train_dataset_size-val_dataset_size])\n",
    "print(train_dataset[0]['conversations'])"
   ],
   "id": "bfc411dd154691ec"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import transformers\n",
    "from typing import Dict, Sequence, List\n",
    "from torch.utils.data import Dataset\n",
    "from dataclasses import dataclass\n",
    "\n",
    "def preprocess(\n",
    "    sources,\n",
    "    tokenizer: transformers.PreTrainedTokenizer,\n",
    ") -> Dict:\n",
    "    template = Template(tokenizer.chat_template)\n",
    "    max_seq_len = tokenizer.model_max_length\n",
    "    messages = []\n",
    "    for i, source in enumerate(sources):\n",
    "        if source[0][\"from\"] != \"human\":\n",
    "            # Skip the first one if it is not from human\n",
    "            source = source[1:]\n",
    "\n",
    "        for j in range(0, len(source), 2):\n",
    "            if j+1 >= len(source): continue\n",
    "            q = source[j][\"value\"]\n",
    "            a = source[j+1][\"value\"]\n",
    "            assert q is not None and a is not None, f'q:{q} a:{a}'\n",
    "            input =  template.render(messages=[{\"role\": \"user\", \"content\": q},{\"role\": \"assistant\", \"content\": a}],bos_token=tokenizer.bos_token,add_generation_prompt=False)\n",
    "            input_ids = tokenizer.encode(input, add_special_tokens= False)\n",
    "\n",
    "            query = template.render(messages=[{\"role\": \"user\", \"content\": q}],bos_token=tokenizer.bos_token,add_generation_prompt=True)\n",
    "            query_ids = tokenizer.encode(query, add_special_tokens= False)\n",
    "\n",
    "            labels = [-100]*len(query_ids) + input_ids[len(query_ids):]\n",
    "            assert len(labels) == len(input_ids)\n",
    "            if len(input_ids) == 0: continue\n",
    "            messages.append({\"input_ids\": input_ids[-max_seq_len:], \"labels\": labels[-max_seq_len:]})\n",
    "\n",
    "    input_ids = [item[\"input_ids\"] for item in messages]\n",
    "    labels = [item[\"labels\"] for item in messages]\n",
    "\n",
    "    max_len = max(len(x) for x in input_ids)\n",
    "\n",
    "    max_len = min(max_len, max_seq_len)\n",
    "    input_ids = [ item[:max_len] + [tokenizer.eos_token_id]*(max_len-len(item)) for item in input_ids]\n",
    "    labels = [ item[:max_len] + [-100]*(max_len-len(item)) for item in labels]\n",
    "\n",
    "    input_ids = torch.LongTensor(input_ids)\n",
    "    labels = torch.LongTensor(labels)\n",
    "    return {\n",
    "        \"input_ids\": input_ids,\n",
    "        \"labels\": labels\n",
    "    }\n",
    "\n",
    "\n",
    "class InstructDataset(Dataset):\n",
    "    def __init__(self, data: Sequence, tokenizer: transformers.PreTrainedTokenizer) -> None:\n",
    "        super().__init__()\n",
    "        self.tokenizer = tokenizer\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index) -> Dict[str, torch.Tensor]:\n",
    "        sources = self.data[index]\n",
    "        if isinstance(index, int):\n",
    "            sources = [sources]\n",
    "        data_dict = preprocess([e['conversations'] for e in sources], self.tokenizer)\n",
    "        if isinstance(index, int):\n",
    "            data_dict = dict(input_ids=data_dict[\"input_ids\"][0], labels=data_dict[\"labels\"][0])\n",
    "        return data_dict\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class DataCollatorForSupervisedDataset(object):\n",
    "    tokenizer: transformers.PreTrainedTokenizer\n",
    "    def __call__(self, instances: Sequence[Dict]) -> Dict[str, torch.Tensor]:\n",
    "        input_ids, labels = tuple([instance[key] for instance in instances] for key in (\"input_ids\", \"labels\"))\n",
    "        input_ids = torch.nn.utils.rnn.pad_sequence(\n",
    "            input_ids,\n",
    "            batch_first=True,\n",
    "            padding_value=self.tokenizer.pad_token_id)\n",
    "        labels = torch.nn.utils.rnn.pad_sequence(labels, batch_first=True, padding_value=IGNORE_INDEX)\n",
    "        return dict(\n",
    "            input_ids=input_ids,\n",
    "            labels=labels,\n",
    "            attention_mask=input_ids.ne(self.tokenizer.pad_token_id),\n",
    "        )"
   ],
   "id": "24b588a917e273c4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "train_dataset = InstructDataset(train_dataset, tokenizer)\n",
    "val_dataset = InstructDataset(val_dataset, tokenizer)\n",
    "data_collator = DataCollatorForSupervisedDataset(tokenizer=tokenizer)"
   ],
   "id": "303cab52a246350d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "sample_data = train_dataset[9]\n",
    "IGNORE_INDEX=-100\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"Debuging: \")\n",
    "print(f\"Input_ids\\n{sample_data['input_ids']}\")\n",
    "print(f\"Label_ids\\n{sample_data['labels']}\")\n",
    "print(\"-\" * 80)\n",
    "print(f\"Input:\\n{tokenizer.decode(sample_data['input_ids'])}\")\n",
    "print(\"-\" * 80)\n",
    "N_id = tokenizer.encode(\"N\", add_special_tokens= False)[0]\n",
    "print(f\"Label:\\n{tokenizer.decode([N_id if x == -100 else x for x in sample_data['labels']])}\")\n",
    "print(\"=\" * 80)\n"
   ],
   "id": "999c1ee24ffde445"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Set training parameters\n",
    "training_arguments = transformers.TrainingArguments(\n",
    "    num_train_epochs=5,\n",
    "    learning_rate=2.5e-5,                  # 微调学习率\n",
    "    lr_scheduler_type=\"cosine_with_restarts\",  # 启用带重启的余弦退火\n",
    "    warmup_ratio=0.1,\n",
    "    weight_decay=0.01,\n",
    ")"
   ],
   "id": "d0355b5b76dd599d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "model.train()\n",
    "trainer = transformers.Trainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_arguments,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    data_collator=data_collator\n",
    ")\n",
    "trainer.train()"
   ],
   "id": "b39bc69e4a1c086"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def print_trainable_parameters(model):\n",
    "    \"\"\"\n",
    "    Prints the number of trainable parameters in the model.\n",
    "    \"\"\"\n",
    "    trainable_params = 0\n",
    "    all_param = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_param += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "    print(\n",
    "        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n",
    "    )\n",
    "\n",
    "model.print_trainable_parameters()"
   ],
   "id": "bb31d80943c47215"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import math\n",
    "!pip install -q -U git+https://github.com/huggingface/accelerate.git\n",
    "eval_results = trainer.evaluate()\n",
    "print(f\"Perplexity: {math.exp(eval_results['eval_loss']):.2f}\")"
   ],
   "id": "82fa0debe288c1b5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "!pwd\n",
    "output_path = \"ilora\"\n",
    "trainer.save_model(output_path)"
   ],
   "id": "24d0399b12303cd5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "template = Template(tokenizer.chat_template)\n",
    "@torch.no_grad()\n",
    "def generate(prompt):\n",
    "    modelInput = template.render(messages=[{\"role\": \"user\", \"content\": prompt}],bos_token= tokenizer.bos_token,add_generation_prompt=True)\n",
    "    input_ids = tokenizer.encode(modelInput, add_special_tokens=False, return_tensors='pt').to(\"cuda:0\")\n",
    "    outputs = model.generate(\n",
    "    input_ids,\n",
    "    do_sample=False,              # 关闭采样\n",
    "    temperature=1.0,              # 无效参数（因do_sample=False）\n",
    "    max_new_tokens=10,            # 限制生成长度（仅需选项字母）\n",
    ")\n",
    "    model_return_string = tokenizer.decode(*outputs, skip_special_tokens=False)\n",
    "    print(\"-\"*80)\n",
    "    print(f\"model_return_string:\\n{model_return_string}\")\n",
    "    generated_ids = outputs[:, input_ids.shape[1]:]\n",
    "    generated_text = tokenizer.decode(generated_ids[0], skip_special_tokens=False)\n",
    "    return generated_text\n",
    "\n",
    "query = \"I get hit\"\n",
    "print(f\"query:\\n{query}\")\n",
    "response = generate(query)\n",
    "print(\"-\"*80)\n",
    "print(f\"response:\\n{response}\")"
   ],
   "id": "5651c62353a0b148"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Empty VRAM\n",
    "# del model\n",
    "# del trainer\n",
    "import gc\n",
    "import torch\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "gc.collect()"
   ],
   "id": "62e5dcb6ca3dbae4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "!nvidia-smi",
   "id": "dceeeed1e391c036"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "@torch.no_grad()\n",
    "def generate(prompts):\n",
    "    model_inputs = [template.render(messages=[{\"role\": \"user\", \"content\": prompt}], bos_token=tokenizer.bos_token, add_generation_prompt=True) for prompt in prompts]\n",
    "    input_ids = tokenizer(model_inputs, add_special_tokens=False, return_tensors='pt', padding=True).to(\"cuda:0\")\n",
    "\n",
    "    outputs = model.generate(input_ids.input_ids, attention_mask=input_ids.attention_mask, max_new_tokens=100)\n",
    "\n",
    "    generated_texts = []\n",
    "    for i in range(len(prompts)):\n",
    "        generated_ids = outputs[i, input_ids.input_ids.shape[1]:]\n",
    "        generated_text = tokenizer.decode(generated_ids, skip_special_tokens=True, clean_up_tokenization_spaces=True)\n",
    "        generated_texts.append(generated_text)\n",
    "\n",
    "    return generated_texts\n",
    "\n",
    "# test\n",
    "print(\"\\n\\n\".join(generate([\"I get hit\", \"Who are you?\"])))"
   ],
   "id": "1e10c225b2f19fe0"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
